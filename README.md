# MediLake Health – EHR Delta Lakehouse Platform

**Overview**

MediLake Health is an end-to-end Healthcare Electronic Health Record (EHR) Delta Lakehouse platform built using Databricks and Delta Lake.  
The project is designed to practice advanced Delta Lake pipelines, performance optimization techniques, and certification-aligned design patterns commonly used in real-world healthcare analytics platforms.

The implementation follows the Medallion Architecture (Bronze–Silver–Gold), reflecting production-grade healthcare data platforms.

---

## Project Objectives
- Build a production-style EHR Data Lakehouse
- Implement Delta Lake MERGE, CDC, and SCD Type-2 patterns
- Apply partitioning, Z-Ordering, OPTIMIZE, and VACUUM
- Create analytics-ready Gold tables
- Prepare for Databricks Data Engineer certification and technical interviews

## Architecture Overview

Raw JSON (EHR Data)
↓
Bronze Layer
(Raw, Append-only)
↓
Silver Layer
(Cleaned, SCD, MERGE)
↓
Gold Layer
(Aggregated, Analytics-ready)


```
medilake-health-ehr-lakehouse/
│
├── data/
│   ├── patients.json            # Patient master data (SCD source)
│   ├── encounters.json          # Hospital visits & admissions
│   ├── diagnoses.json           # Diagnosis details per encounter
│   ├── procedures.json          # Medical procedures performed
│   ├── lab_results.json         # Lab test results (append-only)
│   └── medications.json         # Prescribed medications
│
├── notebooks/
│   ├── 01_bronze_ingestion.ipynb        # Raw JSON → Bronze Delta tables
│   ├── 02_silver_transformations.ipynb  # MERGE, SCD Type-2, data cleansing
│   ├── 03_delta_optimization.ipynb      # Partitioning, Z-ORDER, OPTIMIZE
│   └── 04_gold_analytics.ipynb          # Business metrics & aggregations
│
├── diagrams/
│   └── lakehouse_architecture.png       # Bronze–Silver–Gold architecture diagram (Generated by AI)
│
├── Unity Catalog Design.ipynb
├── Tables Design.ipynb
|
└── README.md                     # Project overview & documentation

```

---

## Bronze Layer – Raw Ingestion

### Purpose
- Store raw EHR data without modification
- Enable replayability, traceability, and auditing

### Key Features
- JSON ingestion using PySpark
- Schema inference
- Append-only Delta tables
- Ingestion timestamp tracking

### Bronze Tables
- bronze.patients
- bronze.encounters
- bronze.diagnoses
- bronze.procedures
- bronze.lab_results
- bronze.medications

---

## Silver Layer – Clean and Conformed Data

### Purpose
- Apply business and data quality rules
- Handle updates, late-arriving records, and CDC
- Create clean dimensions and fact tables

### Key Implementations
- SCD Type-2 for patient demographic changes
- MERGE INTO for CDC handling
- Deduplication using window functions
- Active medication logic

### Silver Tables
- silver.patients_scd
- silver.encounters_clean
- silver.diagnoses_clean
- silver.lab_results_clean
- silver.medications_clean

---

## Delta Lake Performance Optimization

### Techniques Used
- Partitioning on date-based columns
- Z-Ordering on high-cardinality filter columns
- OPTIMIZE for small file compaction
- VACUUM for storage cleanup
- Partition-pruned MERGE operations

### Optimization Examples
- Partition by encounter_date and result_date
- Z-ORDER by patient_id and encounter_id

---

## Gold Layer – Analytics and Reporting

### Purpose
- Provide fast, business-ready datasets
- Minimize joins for BI dashboards and reporting

### Gold Tables
- gold.patient_clinical_summary
- gold.hospital_utilization_metrics
- gold.disease_prevalence_daily
- gold.lab_abnormal_trends

### Use Cases
- Patient-level clinical summaries
- Hospital admission and length-of-stay analysis
- Disease prevalence trend analysis
- Lab abnormality monitoring

---

## Key Concepts Practiced
- Delta Lake ACID transactions
- SCD Type-2 implementation
- MERGE versus INSERT versus OVERWRITE
- Partitioning versus Z-Ordering
- Bronze–Silver–Gold architecture principles
- Exam-style decision-making scenarios

---

## Technologies Used
- Databricks
- Apache Spark (SQL and PySpark)
- Delta Lake
- JSON
- SQL Analytics

---

## Intended Audience
- Aspiring data engineers
- Databricks certification candidates
- Engineers preparing for Lakehouse architecture interviews
- Professionals learning Delta Lake performance optimization

---

## Future Enhancements
- Streaming ingestion using Auto Loader
- Data quality checks and expectations
- Unity Catalog integration
- BI dashboard integration

---

## License
This project is intended for learning and demonstration purposes.

